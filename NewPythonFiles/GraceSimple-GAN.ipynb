{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../dataset-and-plotting/nnPlotting.py:10: MatplotlibDeprecationWarning: \n",
      "The mpl_toolkits.axes_grid module was deprecated in Matplotlib 2.1 and will be removed two minor releases later. Use mpl_toolkits.axes_grid1 and mpl_toolkits.axisartist, which provide the same functionality instead.\n",
      "  from mpl_toolkits.axes_grid.anchored_artists import AnchoredText\n",
      "/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Discriminator Loss: [1.3769336, 0.0] \t\t Generator Loss: 0.0938756912946701\n",
      "Epoch: 1 \t Discriminator Loss: [1.3817214, 0.0] \t\t Generator Loss: 0.09310531616210938\n",
      "Epoch: 2 \t Discriminator Loss: [1.3865819, 0.0] \t\t Generator Loss: 0.09223754703998566\n",
      "Epoch: 3 \t Discriminator Loss: [1.3910187, 0.0] \t\t Generator Loss: 0.09135377407073975\n",
      "Epoch: 4 \t Discriminator Loss: [1.3967855, 0.0] \t\t Generator Loss: 0.09026625752449036\n",
      "Epoch: 5 \t Discriminator Loss: [1.4015301, 0.0] \t\t Generator Loss: 0.08929625153541565\n",
      "Epoch: 6 \t Discriminator Loss: [1.4064267, 0.0] \t\t Generator Loss: 0.08833974599838257\n",
      "Epoch: 7 \t Discriminator Loss: [1.4114841, 0.0] \t\t Generator Loss: 0.08739836513996124\n",
      "Epoch: 8 \t Discriminator Loss: [1.4162548, 0.0] \t\t Generator Loss: 0.08649292588233948\n",
      "Epoch: 9 \t Discriminator Loss: [1.421278, 0.0] \t\t Generator Loss: 0.08556117117404938\n",
      "Epoch: 10 \t Discriminator Loss: [1.4262384, 0.0] \t\t Generator Loss: 0.08465002477169037\n",
      "Epoch: 11 \t Discriminator Loss: [1.4306893, 0.0] \t\t Generator Loss: 0.0838303342461586\n",
      "Epoch: 12 \t Discriminator Loss: [1.4355605, 0.0] \t\t Generator Loss: 0.08294886350631714\n",
      "Epoch: 13 \t Discriminator Loss: [1.4402026, 0.0] \t\t Generator Loss: 0.08211901783943176\n",
      "Epoch: 14 \t Discriminator Loss: [1.445064, 0.0] \t\t Generator Loss: 0.08127541840076447\n",
      "Epoch: 15 \t Discriminator Loss: [1.4494702, 0.0] \t\t Generator Loss: 0.08049667626619339\n",
      "Epoch: 16 \t Discriminator Loss: [1.4539984, 0.0] \t\t Generator Loss: 0.07972066104412079\n",
      "Epoch: 17 \t Discriminator Loss: [1.4583445, 0.0] \t\t Generator Loss: 0.07897993922233582\n",
      "Epoch: 18 \t Discriminator Loss: [1.4633133, 0.0] \t\t Generator Loss: 0.0781487450003624\n",
      "Epoch: 19 \t Discriminator Loss: [1.4674727, 0.0] \t\t Generator Loss: 0.07745324075222015\n",
      "Epoch: 20 \t Discriminator Loss: [1.472049, 0.0] \t\t Generator Loss: 0.07669930160045624\n",
      "Epoch: 21 \t Discriminator Loss: [1.4762188, 0.0] \t\t Generator Loss: 0.07602450996637344\n",
      "Epoch: 22 \t Discriminator Loss: [1.4808241, 0.0] \t\t Generator Loss: 0.07531282305717468\n",
      "Epoch: 23 \t Discriminator Loss: [1.4851992, 0.0] \t\t Generator Loss: 0.07459485530853271\n",
      "Epoch: 24 \t Discriminator Loss: [1.4892337, 0.0] \t\t Generator Loss: 0.07395999878644943\n",
      "Epoch: 25 \t Discriminator Loss: [1.4934416, 0.0] \t\t Generator Loss: 0.07329493761062622\n",
      "Epoch: 26 \t Discriminator Loss: [1.4974022, 0.0] \t\t Generator Loss: 0.07268457114696503\n",
      "Epoch: 27 \t Discriminator Loss: [1.5019727, 0.0] \t\t Generator Loss: 0.07198210060596466\n",
      "Epoch: 28 \t Discriminator Loss: [1.5061231, 0.0] \t\t Generator Loss: 0.07135000824928284\n",
      "Epoch: 29 \t Discriminator Loss: [1.5102143, 0.0] \t\t Generator Loss: 0.07073816657066345\n",
      "Epoch: 30 \t Discriminator Loss: [1.5141251, 0.0] \t\t Generator Loss: 0.07015646249055862\n",
      "Epoch: 31 \t Discriminator Loss: [1.5184317, 0.0] \t\t Generator Loss: 0.06952094286680222\n",
      "Epoch: 32 \t Discriminator Loss: [1.5220044, 0.0] \t\t Generator Loss: 0.06900613009929657\n",
      "Epoch: 33 \t Discriminator Loss: [1.5265254, 0.0] \t\t Generator Loss: 0.06835034489631653\n",
      "Epoch: 34 \t Discriminator Loss: [1.5305198, 0.0] \t\t Generator Loss: 0.06777111440896988\n",
      "Epoch: 35 \t Discriminator Loss: [1.5343181, 0.0] \t\t Generator Loss: 0.06723466515541077\n",
      "Epoch: 36 \t Discriminator Loss: [1.5382209, 0.0] \t\t Generator Loss: 0.06668609380722046\n",
      "Epoch: 37 \t Discriminator Loss: [1.5420218, 0.0] \t\t Generator Loss: 0.0661616325378418\n",
      "Epoch: 38 \t Discriminator Loss: [1.545923, 0.0] \t\t Generator Loss: 0.06561820209026337\n",
      "Epoch: 39 \t Discriminator Loss: [1.5496168, 0.0] \t\t Generator Loss: 0.06511983275413513\n",
      "Epoch: 40 \t Discriminator Loss: [1.5533433, 0.0] \t\t Generator Loss: 0.06460277736186981\n",
      "Epoch: 41 \t Discriminator Loss: [1.55773, 0.0] \t\t Generator Loss: 0.064028799533844\n",
      "Epoch: 42 \t Discriminator Loss: [1.5607526, 0.0] \t\t Generator Loss: 0.06361522525548935\n",
      "Epoch: 43 \t Discriminator Loss: [1.5647047, 0.0] \t\t Generator Loss: 0.06309115886688232\n",
      "Epoch: 44 \t Discriminator Loss: [1.5683342, 0.0] \t\t Generator Loss: 0.06260911375284195\n",
      "Epoch: 45 \t Discriminator Loss: [1.5720011, 0.0] \t\t Generator Loss: 0.062132105231285095\n",
      "Epoch: 46 \t Discriminator Loss: [1.5755558, 0.0] \t\t Generator Loss: 0.061672624200582504\n",
      "Epoch: 47 \t Discriminator Loss: [1.5793211, 0.0] \t\t Generator Loss: 0.06119018420577049\n",
      "Epoch: 48 \t Discriminator Loss: [1.5825921, 0.0] \t\t Generator Loss: 0.06077313423156738\n",
      "Epoch: 49 \t Discriminator Loss: [1.5865166, 0.0] \t\t Generator Loss: 0.06027896702289581\n",
      "Epoch: 50 \t Discriminator Loss: [1.5901139, 0.0] \t\t Generator Loss: 0.0598263218998909\n",
      "Epoch: 51 \t Discriminator Loss: [1.593379, 0.0] \t\t Generator Loss: 0.05942782387137413\n",
      "Epoch: 52 \t Discriminator Loss: [1.5967519, 0.0] \t\t Generator Loss: 0.059008482843637466\n",
      "Epoch: 53 \t Discriminator Loss: [1.6003213, 0.0] \t\t Generator Loss: 0.05856817960739136\n",
      "Epoch: 54 \t Discriminator Loss: [1.6037387, 0.0] \t\t Generator Loss: 0.05815582349896431\n",
      "Epoch: 55 \t Discriminator Loss: [1.6072733, 0.0] \t\t Generator Loss: 0.057729870080947876\n",
      "Epoch: 56 \t Discriminator Loss: [1.6105361, 0.0] \t\t Generator Loss: 0.05733627825975418\n",
      "Epoch: 57 \t Discriminator Loss: [1.6139433, 0.0] \t\t Generator Loss: 0.056937821209430695\n",
      "Epoch: 58 \t Discriminator Loss: [1.6173682, 0.0] \t\t Generator Loss: 0.05652821809053421\n",
      "Epoch: 59 \t Discriminator Loss: [1.6206396, 0.0] \t\t Generator Loss: 0.05614686757326126\n",
      "Epoch: 60 \t Discriminator Loss: [1.623954, 0.0] \t\t Generator Loss: 0.05576081946492195\n",
      "Epoch: 61 \t Discriminator Loss: [1.627135, 0.0] \t\t Generator Loss: 0.05539604276418686\n",
      "Epoch: 62 \t Discriminator Loss: [1.6303701, 0.0] \t\t Generator Loss: 0.05502995476126671\n",
      "Epoch: 63 \t Discriminator Loss: [1.6337911, 0.0] \t\t Generator Loss: 0.054634008556604385\n",
      "Epoch: 64 \t Discriminator Loss: [1.6368457, 0.0] \t\t Generator Loss: 0.05429711192846298\n",
      "Epoch: 65 \t Discriminator Loss: [1.6401184, 0.0] \t\t Generator Loss: 0.05392250046133995\n",
      "Epoch: 66 \t Discriminator Loss: [1.6433604, 0.0] \t\t Generator Loss: 0.05356626212596893\n",
      "Epoch: 67 \t Discriminator Loss: [1.6464734, 0.0] \t\t Generator Loss: 0.05321703106164932\n",
      "Epoch: 68 \t Discriminator Loss: [1.6496642, 0.0] \t\t Generator Loss: 0.052865900099277496\n",
      "Epoch: 69 \t Discriminator Loss: [1.6528319, 0.0] \t\t Generator Loss: 0.0525202676653862\n",
      "Epoch: 70 \t Discriminator Loss: [1.6557965, 0.0] \t\t Generator Loss: 0.052198685705661774\n",
      "Epoch: 71 \t Discriminator Loss: [1.6589694, 0.0] \t\t Generator Loss: 0.051857009530067444\n",
      "Epoch: 72 \t Discriminator Loss: [1.662123, 0.0] \t\t Generator Loss: 0.05152268707752228\n",
      "Epoch: 73 \t Discriminator Loss: [1.6649485, 0.0] \t\t Generator Loss: 0.051220402121543884\n",
      "Epoch: 74 \t Discriminator Loss: [1.6680408, 0.0] \t\t Generator Loss: 0.05089540779590607\n",
      "Epoch: 75 \t Discriminator Loss: [1.6710255, 0.0] \t\t Generator Loss: 0.05057910457253456\n",
      "Epoch: 76 \t Discriminator Loss: [1.6740574, 0.0] \t\t Generator Loss: 0.05026475712656975\n",
      "Epoch: 77 \t Discriminator Loss: [1.6770167, 0.0] \t\t Generator Loss: 0.04995682090520859\n",
      "Epoch: 78 \t Discriminator Loss: [1.680062, 0.0] \t\t Generator Loss: 0.04964476078748703\n",
      "Epoch: 79 \t Discriminator Loss: [1.6828318, 0.0] \t\t Generator Loss: 0.049362022429704666\n",
      "Epoch: 80 \t Discriminator Loss: [1.685902, 0.0] \t\t Generator Loss: 0.04904938489198685\n",
      "Epoch: 81 \t Discriminator Loss: [1.6887491, 0.0] \t\t Generator Loss: 0.0487632118165493\n",
      "Epoch: 82 \t Discriminator Loss: [1.6916096, 0.0] \t\t Generator Loss: 0.04847560450434685\n",
      "Epoch: 83 \t Discriminator Loss: [1.694437, 0.0] \t\t Generator Loss: 0.04819447919726372\n",
      "Epoch: 84 \t Discriminator Loss: [1.6975126, 0.0] \t\t Generator Loss: 0.04788652062416077\n",
      "Epoch: 85 \t Discriminator Loss: [1.7001724, 0.0] \t\t Generator Loss: 0.04762592166662216\n",
      "Epoch: 86 \t Discriminator Loss: [1.703043, 0.0] \t\t Generator Loss: 0.04734577238559723\n",
      "Epoch: 87 \t Discriminator Loss: [1.7058239, 0.0] \t\t Generator Loss: 0.04707210510969162\n",
      "Epoch: 88 \t Discriminator Loss: [1.7085236, 0.0] \t\t Generator Loss: 0.046818554401397705\n",
      "Epoch: 89 \t Discriminator Loss: [1.7113185, 0.0] \t\t Generator Loss: 0.046541668474674225\n",
      "Epoch: 90 \t Discriminator Loss: [1.7139845, 0.0] \t\t Generator Loss: 0.04628651589155197\n",
      "Epoch: 91 \t Discriminator Loss: [1.7169662, 0.0] \t\t Generator Loss: 0.046011973172426224\n",
      "Epoch: 92 \t Discriminator Loss: [1.7196308, 0.0] \t\t Generator Loss: 0.045754145830869675\n",
      "Epoch: 93 \t Discriminator Loss: [1.7223052, 0.0] \t\t Generator Loss: 0.04550131782889366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 \t Discriminator Loss: [1.7250681, 0.0] \t\t Generator Loss: 0.04524170607328415\n",
      "Epoch: 95 \t Discriminator Loss: [1.7276285, 0.0] \t\t Generator Loss: 0.045004282146692276\n",
      "Epoch: 96 \t Discriminator Loss: [1.730252, 0.0] \t\t Generator Loss: 0.0447690486907959\n",
      "Epoch: 97 \t Discriminator Loss: [1.7332165, 0.0] \t\t Generator Loss: 0.04449097439646721\n",
      "Epoch: 98 \t Discriminator Loss: [1.735597, 0.0] \t\t Generator Loss: 0.04427122324705124\n",
      "Epoch: 99 \t Discriminator Loss: [1.7382135, 0.0] \t\t Generator Loss: 0.044033538550138474\n",
      "Epoch: 100 \t Discriminator Loss: [1.740912, 0.0] \t\t Generator Loss: 0.043790288269519806\n",
      "Epoch: 101 \t Discriminator Loss: [1.743549, 0.0] \t\t Generator Loss: 0.04355274885892868\n",
      "Epoch: 102 \t Discriminator Loss: [1.7461753, 0.0] \t\t Generator Loss: 0.043322883546352386\n",
      "Epoch: 103 \t Discriminator Loss: [1.7487552, 0.0] \t\t Generator Loss: 0.043091025203466415\n",
      "Epoch: 104 \t Discriminator Loss: [1.7513707, 0.0] \t\t Generator Loss: 0.042864199727773666\n",
      "Epoch: 105 \t Discriminator Loss: [1.7537774, 0.0] \t\t Generator Loss: 0.04264606535434723\n",
      "Epoch: 106 \t Discriminator Loss: [1.756408, 0.0] \t\t Generator Loss: 0.04241670295596123\n",
      "Epoch: 107 \t Discriminator Loss: [1.7589122, 0.0] \t\t Generator Loss: 0.04220041632652283\n",
      "Epoch: 108 \t Discriminator Loss: [1.7612715, 0.0] \t\t Generator Loss: 0.04199400171637535\n",
      "Epoch: 109 \t Discriminator Loss: [1.7637875, 0.0] \t\t Generator Loss: 0.041777484118938446\n",
      "Epoch: 110 \t Discriminator Loss: [1.7662148, 0.0] \t\t Generator Loss: 0.04157257080078125\n",
      "Epoch: 111 \t Discriminator Loss: [1.7688904, 0.0] \t\t Generator Loss: 0.04134167358279228\n",
      "Epoch: 112 \t Discriminator Loss: [1.7712567, 0.0] \t\t Generator Loss: 0.041142843663692474\n",
      "Epoch: 113 \t Discriminator Loss: [1.7740681, 0.0] \t\t Generator Loss: 0.04091320186853409\n",
      "Epoch: 114 \t Discriminator Loss: [1.776091, 0.0] \t\t Generator Loss: 0.040737979114055634\n",
      "Epoch: 115 \t Discriminator Loss: [1.7786548, 0.0] \t\t Generator Loss: 0.040524307638406754\n",
      "Epoch: 116 \t Discriminator Loss: [1.7809972, 0.0] \t\t Generator Loss: 0.04032677784562111\n",
      "Epoch: 117 \t Discriminator Loss: [1.7833182, 0.0] \t\t Generator Loss: 0.04013504460453987\n",
      "Epoch: 118 \t Discriminator Loss: [1.785753, 0.0] \t\t Generator Loss: 0.03993506729602814\n",
      "Epoch: 119 \t Discriminator Loss: [1.788317, 0.0] \t\t Generator Loss: 0.039725638926029205\n",
      "Epoch: 120 \t Discriminator Loss: [1.7905405, 0.0] \t\t Generator Loss: 0.039544299244880676\n",
      "Epoch: 121 \t Discriminator Loss: [1.7927979, 0.0] \t\t Generator Loss: 0.03936339169740677\n",
      "Epoch: 122 \t Discriminator Loss: [1.7951605, 0.0] \t\t Generator Loss: 0.0391731858253479\n",
      "Epoch: 123 \t Discriminator Loss: [1.7977123, 0.0] \t\t Generator Loss: 0.038969382643699646\n",
      "Epoch: 124 \t Discriminator Loss: [1.7999231, 0.0] \t\t Generator Loss: 0.03879106044769287\n",
      "Epoch: 125 \t Discriminator Loss: [1.8022802, 0.0] \t\t Generator Loss: 0.038606107234954834\n",
      "Epoch: 126 \t Discriminator Loss: [1.8044946, 0.0] \t\t Generator Loss: 0.03842912241816521\n",
      "Epoch: 127 \t Discriminator Loss: [1.8069199, 0.0] \t\t Generator Loss: 0.03824137896299362\n",
      "Epoch: 128 \t Discriminator Loss: [1.8090138, 0.0] \t\t Generator Loss: 0.038074791431427\n",
      "Epoch: 129 \t Discriminator Loss: [1.8114518, 0.0] \t\t Generator Loss: 0.03788512945175171\n",
      "Epoch: 130 \t Discriminator Loss: [1.8135831, 0.0] \t\t Generator Loss: 0.03771995007991791\n",
      "Epoch: 131 \t Discriminator Loss: [1.8158587, 0.0] \t\t Generator Loss: 0.037544600665569305\n",
      "Epoch: 132 \t Discriminator Loss: [1.818074, 0.0] \t\t Generator Loss: 0.03737736493349075\n",
      "Epoch: 133 \t Discriminator Loss: [1.8205082, 0.0] \t\t Generator Loss: 0.03718864172697067\n",
      "Epoch: 134 \t Discriminator Loss: [1.8225418, 0.0] \t\t Generator Loss: 0.03703396022319794\n",
      "Epoch: 135 \t Discriminator Loss: [1.8247645, 0.0] \t\t Generator Loss: 0.03686578571796417\n",
      "Epoch: 136 \t Discriminator Loss: [1.8269387, 0.0] \t\t Generator Loss: 0.03670351207256317\n",
      "Epoch: 137 \t Discriminator Loss: [1.8292114, 0.0] \t\t Generator Loss: 0.03653169795870781\n",
      "Epoch: 138 \t Discriminator Loss: [1.8313452, 0.0] \t\t Generator Loss: 0.03637242317199707\n",
      "Epoch: 139 \t Discriminator Loss: [1.833461, 0.0] \t\t Generator Loss: 0.03621520847082138\n",
      "Epoch: 140 \t Discriminator Loss: [1.8357273, 0.0] \t\t Generator Loss: 0.036047667264938354\n",
      "Epoch: 141 \t Discriminator Loss: [1.8378391, 0.0] \t\t Generator Loss: 0.035892222076654434\n",
      "Epoch: 142 \t Discriminator Loss: [1.8397236, 0.0] \t\t Generator Loss: 0.03575536608695984\n",
      "Epoch: 143 \t Discriminator Loss: [1.8421823, 0.0] \t\t Generator Loss: 0.035574622452259064\n",
      "Epoch: 144 \t Discriminator Loss: [1.844336, 0.0] \t\t Generator Loss: 0.03541833534836769\n",
      "Epoch: 145 \t Discriminator Loss: [1.8462741, 0.0] \t\t Generator Loss: 0.03527812287211418\n",
      "Epoch: 146 \t Discriminator Loss: [1.8481607, 0.0] \t\t Generator Loss: 0.035144656896591187\n",
      "Epoch: 147 \t Discriminator Loss: [1.850572, 0.0] \t\t Generator Loss: 0.03496997058391571\n",
      "Epoch: 148 \t Discriminator Loss: [1.8527386, 0.0] \t\t Generator Loss: 0.03481803834438324\n",
      "Epoch: 149 \t Discriminator Loss: [1.854747, 0.0] \t\t Generator Loss: 0.03467397019267082\n",
      "Epoch: 150 \t Discriminator Loss: [1.8568745, 0.0] \t\t Generator Loss: 0.03452211245894432\n",
      "Epoch: 151 \t Discriminator Loss: [1.8589034, 0.0] \t\t Generator Loss: 0.03437931090593338\n",
      "Epoch: 152 \t Discriminator Loss: [1.8609226, 0.0] \t\t Generator Loss: 0.03423755615949631\n",
      "Epoch: 153 \t Discriminator Loss: [1.863041, 0.0] \t\t Generator Loss: 0.03409344330430031\n",
      "Epoch: 154 \t Discriminator Loss: [1.8651642, 0.0] \t\t Generator Loss: 0.03394390642642975\n",
      "Epoch: 155 \t Discriminator Loss: [1.8670299, 0.0] \t\t Generator Loss: 0.033812813460826874\n",
      "Epoch: 156 \t Discriminator Loss: [1.8690138, 0.0] \t\t Generator Loss: 0.03367867320775986\n",
      "Epoch: 157 \t Discriminator Loss: [1.8711271, 0.0] \t\t Generator Loss: 0.03353087604045868\n",
      "Epoch: 158 \t Discriminator Loss: [1.873178, 0.0] \t\t Generator Loss: 0.03339209407567978\n",
      "Epoch: 159 \t Discriminator Loss: [1.8751085, 0.0] \t\t Generator Loss: 0.03326062485575676\n",
      "Epoch: 160 \t Discriminator Loss: [1.8771052, 0.0] \t\t Generator Loss: 0.03312386944890022\n",
      "Epoch: 161 \t Discriminator Loss: [1.8790724, 0.0] \t\t Generator Loss: 0.032991014420986176\n",
      "Epoch: 162 \t Discriminator Loss: [1.8810565, 0.0] \t\t Generator Loss: 0.03286152333021164\n",
      "Epoch: 163 \t Discriminator Loss: [1.883045, 0.0] \t\t Generator Loss: 0.03272441774606705\n",
      "Epoch: 164 \t Discriminator Loss: [1.8850176, 0.0] \t\t Generator Loss: 0.03259282559156418\n",
      "Epoch: 165 \t Discriminator Loss: [1.8868654, 0.0] \t\t Generator Loss: 0.03247000277042389\n",
      "Epoch: 166 \t Discriminator Loss: [1.8888497, 0.0] \t\t Generator Loss: 0.032338716089725494\n",
      "Epoch: 167 \t Discriminator Loss: [1.8909249, 0.0] \t\t Generator Loss: 0.032203879207372665\n",
      "Epoch: 168 \t Discriminator Loss: [1.8926735, 0.0] \t\t Generator Loss: 0.032087232917547226\n",
      "Epoch: 169 \t Discriminator Loss: [1.8947268, 0.0] \t\t Generator Loss: 0.03195488452911377\n",
      "Epoch: 170 \t Discriminator Loss: [1.8965442, 0.0] \t\t Generator Loss: 0.03183472901582718\n",
      "Epoch: 171 \t Discriminator Loss: [1.8983566, 0.0] \t\t Generator Loss: 0.03171967715024948\n",
      "Epoch: 172 \t Discriminator Loss: [1.9004356, 0.0] \t\t Generator Loss: 0.03158283978700638\n",
      "Epoch: 173 \t Discriminator Loss: [1.9022622, 0.0] \t\t Generator Loss: 0.031465306878089905\n",
      "Epoch: 174 \t Discriminator Loss: [1.9040927, 0.0] \t\t Generator Loss: 0.03134804219007492\n",
      "Epoch: 175 \t Discriminator Loss: [1.9060054, 0.0] \t\t Generator Loss: 0.03123103268444538\n",
      "Epoch: 176 \t Discriminator Loss: [1.9078947, 0.0] \t\t Generator Loss: 0.03110574185848236\n",
      "Epoch: 177 \t Discriminator Loss: [1.9098473, 0.0] \t\t Generator Loss: 0.03098205476999283\n",
      "Epoch: 178 \t Discriminator Loss: [1.9116126, 0.0] \t\t Generator Loss: 0.03087073564529419\n",
      "Epoch: 179 \t Discriminator Loss: [1.913522, 0.0] \t\t Generator Loss: 0.030750766396522522\n",
      "Epoch: 180 \t Discriminator Loss: [1.9153765, 0.0] \t\t Generator Loss: 0.03063463792204857\n",
      "Epoch: 181 \t Discriminator Loss: [1.9171929, 0.0] \t\t Generator Loss: 0.030521418899297714\n",
      "Epoch: 182 \t Discriminator Loss: [1.9189744, 0.0] \t\t Generator Loss: 0.030410677194595337\n",
      "Epoch: 183 \t Discriminator Loss: [1.9208636, 0.0] \t\t Generator Loss: 0.03029753640294075\n",
      "Epoch: 184 \t Discriminator Loss: [1.9226646, 0.0] \t\t Generator Loss: 0.030185207724571228\n",
      "Epoch: 185 \t Discriminator Loss: [1.924484, 0.0] \t\t Generator Loss: 0.030070967972278595\n",
      "Epoch: 186 \t Discriminator Loss: [1.926188, 0.0] \t\t Generator Loss: 0.029969101771712303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 187 \t Discriminator Loss: [1.9280922, 0.0] \t\t Generator Loss: 0.029851801693439484\n",
      "Epoch: 188 \t Discriminator Loss: [1.9297957, 0.0] \t\t Generator Loss: 0.029747091233730316\n",
      "Epoch: 189 \t Discriminator Loss: [1.9315484, 0.0] \t\t Generator Loss: 0.029642250388860703\n",
      "Epoch: 190 \t Discriminator Loss: [1.9333154, 0.0] \t\t Generator Loss: 0.029535692185163498\n",
      "Epoch: 191 \t Discriminator Loss: [1.9351673, 0.0] \t\t Generator Loss: 0.029423128813505173\n",
      "Epoch: 192 \t Discriminator Loss: [1.9369245, 0.0] \t\t Generator Loss: 0.029317986220121384\n",
      "Epoch: 193 \t Discriminator Loss: [1.9387162, 0.0] \t\t Generator Loss: 0.029211152344942093\n",
      "Epoch: 194 \t Discriminator Loss: [1.9404733, 0.0] \t\t Generator Loss: 0.029106751084327698\n",
      "Epoch: 195 \t Discriminator Loss: [1.94224, 0.0] \t\t Generator Loss: 0.029002144932746887\n",
      "Epoch: 196 \t Discriminator Loss: [1.9441485, 0.0] \t\t Generator Loss: 0.02889210358262062\n",
      "Epoch: 197 \t Discriminator Loss: [1.9457107, 0.0] \t\t Generator Loss: 0.028797835111618042\n",
      "Epoch: 198 \t Discriminator Loss: [1.9473728, 0.0] \t\t Generator Loss: 0.02870168536901474\n",
      "Epoch: 199 \t Discriminator Loss: [1.9491246, 0.0] \t\t Generator Loss: 0.028598248958587646\n",
      "Epoch: 200 \t Discriminator Loss: [1.95085, 0.0] \t\t Generator Loss: 0.028497949242591858\n",
      "Epoch: 201 \t Discriminator Loss: [1.9525886, 0.0] \t\t Generator Loss: 0.028397202491760254\n",
      "Epoch: 202 \t Discriminator Loss: [1.9541838, 0.0] \t\t Generator Loss: 0.028305066749453545\n",
      "Epoch: 203 \t Discriminator Loss: [1.955991, 0.0] \t\t Generator Loss: 0.028201047331094742\n",
      "Epoch: 204 \t Discriminator Loss: [1.9575777, 0.0] \t\t Generator Loss: 0.028112394735217094\n",
      "Epoch: 205 \t Discriminator Loss: [1.9593608, 0.0] \t\t Generator Loss: 0.028008179739117622\n",
      "Epoch: 206 \t Discriminator Loss: [1.9610372, 0.0] \t\t Generator Loss: 0.027912773191928864\n",
      "Epoch: 207 \t Discriminator Loss: [1.9625992, 0.0] \t\t Generator Loss: 0.027825288474559784\n",
      "Epoch: 208 \t Discriminator Loss: [1.9643998, 0.0] \t\t Generator Loss: 0.02772234007716179\n",
      "Epoch: 209 \t Discriminator Loss: [1.966074, 0.0] \t\t Generator Loss: 0.027628067880868912\n",
      "Epoch: 210 \t Discriminator Loss: [1.967735, 0.0] \t\t Generator Loss: 0.027534805238246918\n",
      "Epoch: 211 \t Discriminator Loss: [1.9694179, 0.0] \t\t Generator Loss: 0.027445483952760696\n",
      "Epoch: 212 \t Discriminator Loss: [1.9710557, 0.0] \t\t Generator Loss: 0.027349306270480156\n",
      "Epoch: 213 \t Discriminator Loss: [1.9726824, 0.0] \t\t Generator Loss: 0.02725890651345253\n",
      "Epoch: 214 \t Discriminator Loss: [1.9743768, 0.0] \t\t Generator Loss: 0.027165081351995468\n",
      "Epoch: 215 \t Discriminator Loss: [1.9760606, 0.0] \t\t Generator Loss: 0.027072206139564514\n",
      "Epoch: 216 \t Discriminator Loss: [1.9775572, 0.0] \t\t Generator Loss: 0.02698983997106552\n",
      "Epoch: 217 \t Discriminator Loss: [1.97928, 0.0] \t\t Generator Loss: 0.026896726340055466\n",
      "Epoch: 218 \t Discriminator Loss: [1.9807947, 0.0] \t\t Generator Loss: 0.026812637224793434\n",
      "Epoch: 219 \t Discriminator Loss: [1.982486, 0.0] \t\t Generator Loss: 0.026720529422163963\n",
      "Epoch: 220 \t Discriminator Loss: [1.9840972, 0.0] \t\t Generator Loss: 0.026634253561496735\n",
      "Epoch: 221 \t Discriminator Loss: [1.9857949, 0.0] \t\t Generator Loss: 0.026541294530034065\n",
      "Epoch: 222 \t Discriminator Loss: [1.9871573, 0.0] \t\t Generator Loss: 0.026467811316251755\n",
      "Epoch: 223 \t Discriminator Loss: [1.9888551, 0.0] \t\t Generator Loss: 0.026379752904176712\n",
      "Epoch: 224 \t Discriminator Loss: [1.9905336, 0.0] \t\t Generator Loss: 0.026286741718649864\n",
      "Epoch: 225 \t Discriminator Loss: [1.9919176, 0.0] \t\t Generator Loss: 0.026212790980935097\n",
      "Epoch: 226 \t Discriminator Loss: [1.9935102, 0.0] \t\t Generator Loss: 0.02612917870283127\n",
      "Epoch: 227 \t Discriminator Loss: [1.995224, 0.0] \t\t Generator Loss: 0.026037191972136497\n",
      "Epoch: 228 \t Discriminator Loss: [1.9967144, 0.0] \t\t Generator Loss: 0.025958359241485596\n",
      "Epoch: 229 \t Discriminator Loss: [1.9982305, 0.0] \t\t Generator Loss: 0.025881770998239517\n",
      "Epoch: 230 \t Discriminator Loss: [1.9997863, 0.0] \t\t Generator Loss: 0.025796715170145035\n",
      "Epoch: 231 \t Discriminator Loss: [2.0012863, 0.0] \t\t Generator Loss: 0.025718145072460175\n",
      "Epoch: 232 \t Discriminator Loss: [2.0028791, 0.0] \t\t Generator Loss: 0.025635451078414917\n",
      "Epoch: 233 \t Discriminator Loss: [2.004459, 0.0] \t\t Generator Loss: 0.025552768260240555\n",
      "Epoch: 234 \t Discriminator Loss: [2.0059476, 0.0] \t\t Generator Loss: 0.025475529953837395\n",
      "Epoch: 235 \t Discriminator Loss: [2.007679, 0.0] \t\t Generator Loss: 0.02538761869072914\n",
      "Epoch: 236 \t Discriminator Loss: [2.0090468, 0.0] \t\t Generator Loss: 0.025315556675195694\n",
      "Epoch: 237 \t Discriminator Loss: [2.0106578, 0.0] \t\t Generator Loss: 0.025232817977666855\n",
      "Epoch: 238 \t Discriminator Loss: [2.012292, 0.0] \t\t Generator Loss: 0.025152191519737244\n",
      "Epoch: 239 \t Discriminator Loss: [2.0134263, 0.0] \t\t Generator Loss: 0.025091126561164856\n",
      "Epoch: 240 \t Discriminator Loss: [2.015089, 0.0] \t\t Generator Loss: 0.025007590651512146\n",
      "Epoch: 241 \t Discriminator Loss: [2.016709, 0.0] \t\t Generator Loss: 0.02492433227598667\n",
      "Epoch: 242 \t Discriminator Loss: [2.0182142, 0.0] \t\t Generator Loss: 0.024848222732543945\n",
      "Epoch: 243 \t Discriminator Loss: [2.0197043, 0.0] \t\t Generator Loss: 0.024773094803094864\n",
      "Epoch: 244 \t Discriminator Loss: [2.0210726, 0.0] \t\t Generator Loss: 0.024705354124307632\n",
      "Epoch: 245 \t Discriminator Loss: [2.0226445, 0.0] \t\t Generator Loss: 0.024625521153211594\n",
      "Epoch: 246 \t Discriminator Loss: [2.0241318, 0.0] \t\t Generator Loss: 0.024554383009672165\n",
      "Epoch: 247 \t Discriminator Loss: [2.0258014, 0.0] \t\t Generator Loss: 0.024474039673805237\n",
      "Epoch: 248 \t Discriminator Loss: [2.0270576, 0.0] \t\t Generator Loss: 0.024406811222434044\n",
      "Epoch: 249 \t Discriminator Loss: [2.028465, 0.0] \t\t Generator Loss: 0.02433604933321476\n",
      "Epoch: 250 \t Discriminator Loss: [2.0301523, 0.0] \t\t Generator Loss: 0.02425704337656498\n",
      "Epoch: 251 \t Discriminator Loss: [2.0314996, 0.0] \t\t Generator Loss: 0.024187985807657242\n",
      "Epoch: 252 \t Discriminator Loss: [2.0330098, 0.0] \t\t Generator Loss: 0.02411392331123352\n",
      "Epoch: 253 \t Discriminator Loss: [2.0343723, 0.0] \t\t Generator Loss: 0.024045798927545547\n",
      "Epoch: 254 \t Discriminator Loss: [2.0357575, 0.0] \t\t Generator Loss: 0.023978255689144135\n",
      "Epoch: 255 \t Discriminator Loss: [2.037208, 0.0] \t\t Generator Loss: 0.023907708004117012\n",
      "Epoch: 256 \t Discriminator Loss: [2.0387273, 0.0] \t\t Generator Loss: 0.023834090679883957\n",
      "Epoch: 257 \t Discriminator Loss: [2.0401745, 0.0] \t\t Generator Loss: 0.02376517653465271\n",
      "Epoch: 258 \t Discriminator Loss: [2.0415478, 0.0] \t\t Generator Loss: 0.023697957396507263\n",
      "Epoch: 259 \t Discriminator Loss: [2.0429287, 0.0] \t\t Generator Loss: 0.023631587624549866\n",
      "Epoch: 260 \t Discriminator Loss: [2.044474, 0.0] \t\t Generator Loss: 0.023557476699352264\n",
      "Epoch: 261 \t Discriminator Loss: [2.0458686, 0.0] \t\t Generator Loss: 0.023490963503718376\n",
      "Epoch: 262 \t Discriminator Loss: [2.0472217, 0.0] \t\t Generator Loss: 0.02342652529478073\n",
      "Epoch: 263 \t Discriminator Loss: [2.048743, 0.0] \t\t Generator Loss: 0.02335427515208721\n",
      "Epoch: 264 \t Discriminator Loss: [2.0501347, 0.0] \t\t Generator Loss: 0.023288391530513763\n",
      "Epoch: 265 \t Discriminator Loss: [2.0515113, 0.0] \t\t Generator Loss: 0.023223426192998886\n",
      "Epoch: 266 \t Discriminator Loss: [2.0528035, 0.0] \t\t Generator Loss: 0.02316255122423172\n",
      "Epoch: 267 \t Discriminator Loss: [2.0542333, 0.0] \t\t Generator Loss: 0.023095425218343735\n",
      "Epoch: 268 \t Discriminator Loss: [2.0555668, 0.0] \t\t Generator Loss: 0.02303300052881241\n",
      "Epoch: 269 \t Discriminator Loss: [2.0570776, 0.0] \t\t Generator Loss: 0.02296442538499832\n",
      "Epoch: 270 \t Discriminator Loss: [2.058443, 0.0] \t\t Generator Loss: 0.022898927330970764\n",
      "Epoch: 271 \t Discriminator Loss: [2.0597794, 0.0] \t\t Generator Loss: 0.022836901247501373\n",
      "Epoch: 272 \t Discriminator Loss: [2.061065, 0.0] \t\t Generator Loss: 0.022777363657951355\n",
      "Epoch: 273 \t Discriminator Loss: [2.0626585, 0.0] \t\t Generator Loss: 0.02270526997745037\n",
      "Epoch: 274 \t Discriminator Loss: [2.0639868, 0.0] \t\t Generator Loss: 0.022642750293016434\n",
      "Epoch: 275 \t Discriminator Loss: [2.0654309, 0.0] \t\t Generator Loss: 0.022577885538339615\n",
      "Epoch: 276 \t Discriminator Loss: [2.0666637, 0.0] \t\t Generator Loss: 0.022521141916513443\n",
      "Epoch: 277 \t Discriminator Loss: [2.068089, 0.0] \t\t Generator Loss: 0.022455085068941116\n",
      "Epoch: 278 \t Discriminator Loss: [2.0693517, 0.0] \t\t Generator Loss: 0.02239760011434555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 279 \t Discriminator Loss: [2.0706503, 0.0] \t\t Generator Loss: 0.022338654845952988\n",
      "Epoch: 280 \t Discriminator Loss: [2.0720508, 0.0] \t\t Generator Loss: 0.022276228293776512\n",
      "Epoch: 281 \t Discriminator Loss: [2.0734534, 0.0] \t\t Generator Loss: 0.022211946547031403\n",
      "Epoch: 282 \t Discriminator Loss: [2.0747402, 0.0] \t\t Generator Loss: 0.0221550352871418\n",
      "Epoch: 283 \t Discriminator Loss: [2.07617, 0.0] \t\t Generator Loss: 0.02208993211388588\n",
      "Epoch: 284 \t Discriminator Loss: [2.0774713, 0.0] \t\t Generator Loss: 0.022031698375940323\n",
      "Epoch: 285 \t Discriminator Loss: [2.0787344, 0.0] \t\t Generator Loss: 0.021975301206111908\n",
      "Epoch: 286 \t Discriminator Loss: [2.080049, 0.0] \t\t Generator Loss: 0.021916784346103668\n",
      "Epoch: 287 \t Discriminator Loss: [2.0814576, 0.0] \t\t Generator Loss: 0.021855512633919716\n",
      "Epoch: 288 \t Discriminator Loss: [2.0827763, 0.0] \t\t Generator Loss: 0.021795891225337982\n",
      "Epoch: 289 \t Discriminator Loss: [2.0840096, 0.0] \t\t Generator Loss: 0.02174142561852932\n",
      "Epoch: 290 \t Discriminator Loss: [2.0854006, 0.0] \t\t Generator Loss: 0.0216801967471838\n",
      "Epoch: 291 \t Discriminator Loss: [2.0867052, 0.0] \t\t Generator Loss: 0.021623853594064713\n",
      "Epoch: 292 \t Discriminator Loss: [2.0879087, 0.0] \t\t Generator Loss: 0.0215720497071743\n",
      "Epoch: 293 \t Discriminator Loss: [2.089365, 0.0] \t\t Generator Loss: 0.02150658890604973\n",
      "Epoch: 294 \t Discriminator Loss: [2.0906193, 0.0] \t\t Generator Loss: 0.021453283727169037\n",
      "Epoch: 295 \t Discriminator Loss: [2.0918941, 0.0] \t\t Generator Loss: 0.021396588534116745\n",
      "Epoch: 296 \t Discriminator Loss: [2.0932462, 0.0] \t\t Generator Loss: 0.02133931964635849\n",
      "Epoch: 297 \t Discriminator Loss: [2.0943744, 0.0] \t\t Generator Loss: 0.021289220079779625\n",
      "Epoch: 298 \t Discriminator Loss: [2.0957136, 0.0] \t\t Generator Loss: 0.021232444792985916\n",
      "Epoch: 299 \t Discriminator Loss: [2.0971284, 0.0] \t\t Generator Loss: 0.02117333561182022\n",
      "Epoch: 300 \t Discriminator Loss: [2.0984175, 0.0] \t\t Generator Loss: 0.02111675590276718\n",
      "Epoch: 301 \t Discriminator Loss: [2.099666, 0.0] \t\t Generator Loss: 0.021063348278403282\n",
      "Epoch: 302 \t Discriminator Loss: [2.1008072, 0.0] \t\t Generator Loss: 0.021013403311371803\n",
      "Epoch: 303 \t Discriminator Loss: [2.1021802, 0.0] \t\t Generator Loss: 0.020956292748451233\n",
      "Epoch: 304 \t Discriminator Loss: [2.1033459, 0.0] \t\t Generator Loss: 0.020906854420900345\n",
      "Epoch: 305 \t Discriminator Loss: [2.1046896, 0.0] \t\t Generator Loss: 0.020848680287599564\n",
      "Epoch: 306 \t Discriminator Loss: [2.1058989, 0.0] \t\t Generator Loss: 0.02079763635993004\n",
      "Epoch: 307 \t Discriminator Loss: [2.1072516, 0.0] \t\t Generator Loss: 0.020740728825330734\n",
      "Epoch: 308 \t Discriminator Loss: [2.1084511, 0.0] \t\t Generator Loss: 0.02069036476314068\n",
      "Epoch: 309 \t Discriminator Loss: [2.1098218, 0.0] \t\t Generator Loss: 0.02063671499490738\n",
      "Epoch: 310 \t Discriminator Loss: [2.110854, 0.0] \t\t Generator Loss: 0.020589835941791534\n",
      "Epoch: 311 \t Discriminator Loss: [2.11209, 0.0] \t\t Generator Loss: 0.020538317039608955\n",
      "Epoch: 312 \t Discriminator Loss: [2.1134074, 0.0] \t\t Generator Loss: 0.020484816282987595\n",
      "Epoch: 313 \t Discriminator Loss: [2.1146617, 0.0] \t\t Generator Loss: 0.02043156698346138\n",
      "Epoch: 314 \t Discriminator Loss: [2.115869, 0.0] \t\t Generator Loss: 0.02038252167403698\n",
      "Epoch: 315 \t Discriminator Loss: [2.1170805, 0.0] \t\t Generator Loss: 0.020331669598817825\n",
      "Epoch: 316 \t Discriminator Loss: [2.1184554, 0.0] \t\t Generator Loss: 0.020275140181183815\n",
      "Epoch: 317 \t Discriminator Loss: [2.1194634, 0.0] \t\t Generator Loss: 0.02023373544216156\n",
      "Epoch: 318 \t Discriminator Loss: [2.1208215, 0.0] \t\t Generator Loss: 0.02017940767109394\n",
      "Epoch: 319 \t Discriminator Loss: [2.1220257, 0.0] \t\t Generator Loss: 0.02012898214161396\n",
      "Epoch: 320 \t Discriminator Loss: [2.123097, 0.0] \t\t Generator Loss: 0.020087044686079025\n",
      "Epoch: 321 \t Discriminator Loss: [2.1245356, 0.0] \t\t Generator Loss: 0.020029379054903984\n",
      "Epoch: 322 \t Discriminator Loss: [2.1256552, 0.0] \t\t Generator Loss: 0.019981522113084793\n",
      "Epoch: 323 \t Discriminator Loss: [2.1269474, 0.0] \t\t Generator Loss: 0.019932975992560387\n",
      "Epoch: 324 \t Discriminator Loss: [2.1280417, 0.0] \t\t Generator Loss: 0.019885119050741196\n",
      "Epoch: 325 \t Discriminator Loss: [2.1291964, 0.0] \t\t Generator Loss: 0.019838662818074226\n",
      "Epoch: 326 \t Discriminator Loss: [2.1304522, 0.0] \t\t Generator Loss: 0.01978914812207222\n",
      "Epoch: 327 \t Discriminator Loss: [2.1316347, 0.0] \t\t Generator Loss: 0.019740931689739227\n",
      "Epoch: 328 \t Discriminator Loss: [2.1328464, 0.0] \t\t Generator Loss: 0.019692543894052505\n",
      "Epoch: 329 \t Discriminator Loss: [2.1341014, 0.0] \t\t Generator Loss: 0.01964256353676319\n",
      "Epoch: 330 \t Discriminator Loss: [2.1351805, 0.0] \t\t Generator Loss: 0.019599653780460358\n",
      "Epoch: 331 \t Discriminator Loss: [2.1363907, 0.0] \t\t Generator Loss: 0.019553430378437042\n",
      "Epoch: 332 \t Discriminator Loss: [2.1375637, 0.0] \t\t Generator Loss: 0.019505295902490616\n",
      "Epoch: 333 \t Discriminator Loss: [2.1388366, 0.0] \t\t Generator Loss: 0.019456813111901283\n",
      "Epoch: 334 \t Discriminator Loss: [2.1400127, 0.0] \t\t Generator Loss: 0.019410014152526855\n",
      "Epoch: 335 \t Discriminator Loss: [2.1410851, 0.0] \t\t Generator Loss: 0.019366689026355743\n",
      "Epoch: 336 \t Discriminator Loss: [2.1422896, 0.0] \t\t Generator Loss: 0.01931951195001602\n",
      "Epoch: 337 \t Discriminator Loss: [2.143495, 0.0] \t\t Generator Loss: 0.01927414909005165\n",
      "Epoch: 338 \t Discriminator Loss: [2.1447096, 0.0] \t\t Generator Loss: 0.0192251019179821\n",
      "Epoch: 339 \t Discriminator Loss: [2.145658, 0.0] \t\t Generator Loss: 0.01918818987905979\n",
      "Epoch: 340 \t Discriminator Loss: [2.146903, 0.0] \t\t Generator Loss: 0.01913987100124359\n",
      "Epoch: 341 \t Discriminator Loss: [2.1480896, 0.0] \t\t Generator Loss: 0.019093958660960197\n",
      "Epoch: 342 \t Discriminator Loss: [2.1492915, 0.0] \t\t Generator Loss: 0.019050098955631256\n",
      "Epoch: 343 \t Discriminator Loss: [2.1504414, 0.0] \t\t Generator Loss: 0.019003253430128098\n",
      "Epoch: 344 \t Discriminator Loss: [2.1515696, 0.0] \t\t Generator Loss: 0.018959898501634598\n",
      "Epoch: 345 \t Discriminator Loss: [2.152645, 0.0] \t\t Generator Loss: 0.018919475376605988\n",
      "Epoch: 346 \t Discriminator Loss: [2.1538155, 0.0] \t\t Generator Loss: 0.018874729052186012\n",
      "Epoch: 347 \t Discriminator Loss: [2.1549997, 0.0] \t\t Generator Loss: 0.01882867142558098\n",
      "Epoch: 348 \t Discriminator Loss: [2.1561177, 0.0] \t\t Generator Loss: 0.018786128610372543\n",
      "Epoch: 349 \t Discriminator Loss: [2.157267, 0.0] \t\t Generator Loss: 0.018742460757493973\n",
      "Epoch: 350 \t Discriminator Loss: [2.1584086, 0.0] \t\t Generator Loss: 0.018703363835811615\n",
      "Epoch: 351 \t Discriminator Loss: [2.1594858, 0.0] \t\t Generator Loss: 0.018658466637134552\n",
      "Epoch: 352 \t Discriminator Loss: [2.1606503, 0.0] \t\t Generator Loss: 0.018616169691085815\n",
      "Epoch: 353 \t Discriminator Loss: [2.1616964, 0.0] \t\t Generator Loss: 0.018575165420770645\n",
      "Epoch: 354 \t Discriminator Loss: [2.162785, 0.0] \t\t Generator Loss: 0.01853427104651928\n",
      "Epoch: 355 \t Discriminator Loss: [2.164024, 0.0] \t\t Generator Loss: 0.018487853929400444\n",
      "Epoch: 356 \t Discriminator Loss: [2.1650457, 0.0] \t\t Generator Loss: 0.018449656665325165\n",
      "Epoch: 357 \t Discriminator Loss: [2.1662607, 0.0] \t\t Generator Loss: 0.018404360860586166\n",
      "Epoch: 358 \t Discriminator Loss: [2.1674664, 0.0] \t\t Generator Loss: 0.018362000584602356\n",
      "Epoch: 359 \t Discriminator Loss: [2.168466, 0.0] \t\t Generator Loss: 0.01832238957285881\n",
      "Epoch: 360 \t Discriminator Loss: [2.1696434, 0.0] \t\t Generator Loss: 0.01827995665371418\n",
      "Epoch: 361 \t Discriminator Loss: [2.1706495, 0.0] \t\t Generator Loss: 0.01824161782860756\n",
      "Epoch: 362 \t Discriminator Loss: [2.1717973, 0.0] \t\t Generator Loss: 0.018199287354946136\n",
      "Epoch: 363 \t Discriminator Loss: [2.1728773, 0.0] \t\t Generator Loss: 0.018159573897719383\n",
      "Epoch: 364 \t Discriminator Loss: [2.1739697, 0.0] \t\t Generator Loss: 0.01811947301030159\n",
      "Epoch: 365 \t Discriminator Loss: [2.1750407, 0.0] \t\t Generator Loss: 0.018080243840813637\n",
      "Epoch: 366 \t Discriminator Loss: [2.1762252, 0.0] \t\t Generator Loss: 0.01803777739405632\n",
      "Epoch: 367 \t Discriminator Loss: [2.1772819, 0.0] \t\t Generator Loss: 0.018000073730945587\n",
      "Epoch: 368 \t Discriminator Loss: [2.1783729, 0.0] \t\t Generator Loss: 0.0179587509483099\n",
      "Epoch: 369 \t Discriminator Loss: [2.1794858, 0.0] \t\t Generator Loss: 0.01791916787624359\n",
      "Epoch: 370 \t Discriminator Loss: [2.1805682, 0.0] \t\t Generator Loss: 0.017880316823720932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 371 \t Discriminator Loss: [2.1815038, 0.0] \t\t Generator Loss: 0.01784535124897957\n",
      "Epoch: 372 \t Discriminator Loss: [2.1826277, 0.0] \t\t Generator Loss: 0.01780479960143566\n",
      "Epoch: 373 \t Discriminator Loss: [2.183701, 0.0] \t\t Generator Loss: 0.017766965553164482\n",
      "Epoch: 374 \t Discriminator Loss: [2.1847758, 0.0] \t\t Generator Loss: 0.017727592960000038\n",
      "Epoch: 375 \t Discriminator Loss: [2.1858788, 0.0] \t\t Generator Loss: 0.01768886111676693\n",
      "Epoch: 376 \t Discriminator Loss: [2.186984, 0.0] \t\t Generator Loss: 0.017648592591285706\n",
      "Epoch: 377 \t Discriminator Loss: [2.188026, 0.0] \t\t Generator Loss: 0.017611421644687653\n",
      "Epoch: 378 \t Discriminator Loss: [2.1892183, 0.0] \t\t Generator Loss: 0.017570141702890396\n",
      "Epoch: 379 \t Discriminator Loss: [2.1901805, 0.0] \t\t Generator Loss: 0.017536427825689316\n",
      "Epoch: 380 \t Discriminator Loss: [2.1912246, 0.0] \t\t Generator Loss: 0.01749786175787449\n",
      "Epoch: 381 \t Discriminator Loss: [2.1922603, 0.0] \t\t Generator Loss: 0.01746123470366001\n",
      "Epoch: 382 \t Discriminator Loss: [2.1932502, 0.0] \t\t Generator Loss: 0.017426298931241035\n",
      "Epoch: 383 \t Discriminator Loss: [2.194359, 0.0] \t\t Generator Loss: 0.01738802343606949\n",
      "Epoch: 384 \t Discriminator Loss: [2.1954186, 0.0] \t\t Generator Loss: 0.017350029200315475\n",
      "Epoch: 385 \t Discriminator Loss: [2.1965156, 0.0] \t\t Generator Loss: 0.017313139513134956\n",
      "Epoch: 386 \t Discriminator Loss: [2.197547, 0.0] \t\t Generator Loss: 0.017277054488658905\n",
      "Epoch: 387 \t Discriminator Loss: [2.1985846, 0.0] \t\t Generator Loss: 0.017240069806575775\n",
      "Epoch: 388 \t Discriminator Loss: [2.199645, 0.0] \t\t Generator Loss: 0.017203129827976227\n",
      "Epoch: 389 \t Discriminator Loss: [2.200759, 0.0] \t\t Generator Loss: 0.017163652926683426\n",
      "Epoch: 390 \t Discriminator Loss: [2.2017188, 0.0] \t\t Generator Loss: 0.017130374908447266\n",
      "Epoch: 391 \t Discriminator Loss: [2.2027705, 0.0] \t\t Generator Loss: 0.017096277326345444\n",
      "Epoch: 392 \t Discriminator Loss: [2.2038326, 0.0] \t\t Generator Loss: 0.017057310789823532\n",
      "Epoch: 393 \t Discriminator Loss: [2.2049012, 0.0] \t\t Generator Loss: 0.017022032290697098\n",
      "Epoch: 394 \t Discriminator Loss: [2.205842, 0.0] \t\t Generator Loss: 0.016988132148981094\n",
      "Epoch: 395 \t Discriminator Loss: [2.2068267, 0.0] \t\t Generator Loss: 0.016955852508544922\n",
      "Epoch: 396 \t Discriminator Loss: [2.207877, 0.0] \t\t Generator Loss: 0.016918374225497246\n",
      "Epoch: 397 \t Discriminator Loss: [2.2089732, 0.0] \t\t Generator Loss: 0.016881683841347694\n",
      "Epoch: 398 \t Discriminator Loss: [2.2099652, 0.0] \t\t Generator Loss: 0.016847025603055954\n",
      "Epoch: 399 \t Discriminator Loss: [2.2109907, 0.0] \t\t Generator Loss: 0.01681220531463623\n",
      "Epoch: 400 \t Discriminator Loss: [2.2119596, 0.0] \t\t Generator Loss: 0.01677929423749447\n",
      "Epoch: 401 \t Discriminator Loss: [2.2131066, 0.0] \t\t Generator Loss: 0.016742592677474022\n",
      "Epoch: 402 \t Discriminator Loss: [2.2140675, 0.0] \t\t Generator Loss: 0.016707956790924072\n",
      "Epoch: 403 \t Discriminator Loss: [2.2150147, 0.0] \t\t Generator Loss: 0.016676750034093857\n",
      "Epoch: 404 \t Discriminator Loss: [2.2160301, 0.0] \t\t Generator Loss: 0.016642536967992783\n",
      "Epoch: 405 \t Discriminator Loss: [2.2170548, 0.0] \t\t Generator Loss: 0.016607332974672318\n",
      "Epoch: 406 \t Discriminator Loss: [2.2180827, 0.0] \t\t Generator Loss: 0.016572877764701843\n",
      "Epoch: 407 \t Discriminator Loss: [2.2190247, 0.0] \t\t Generator Loss: 0.016541339457035065\n",
      "Epoch: 408 \t Discriminator Loss: [2.2200503, 0.0] \t\t Generator Loss: 0.016507066786289215\n",
      "Epoch: 409 \t Discriminator Loss: [2.2211204, 0.0] \t\t Generator Loss: 0.016472481191158295\n",
      "Epoch: 410 \t Discriminator Loss: [2.2220118, 0.0] \t\t Generator Loss: 0.01644175872206688\n",
      "Epoch: 411 \t Discriminator Loss: [2.2230763, 0.0] \t\t Generator Loss: 0.016406424343585968\n",
      "Epoch: 412 \t Discriminator Loss: [2.22404, 0.0] \t\t Generator Loss: 0.01637599617242813\n",
      "Epoch: 413 \t Discriminator Loss: [2.224959, 0.0] \t\t Generator Loss: 0.01634410209953785\n",
      "Epoch: 414 \t Discriminator Loss: [2.226038, 0.0] \t\t Generator Loss: 0.0163084976375103\n",
      "Epoch: 415 \t Discriminator Loss: [2.2270255, 0.0] \t\t Generator Loss: 0.016275975853204727\n",
      "Epoch: 416 \t Discriminator Loss: [2.2279444, 0.0] \t\t Generator Loss: 0.016245771199464798\n",
      "Epoch: 417 \t Discriminator Loss: [2.2289836, 0.0] \t\t Generator Loss: 0.016212712973356247\n",
      "Epoch: 418 \t Discriminator Loss: [2.229998, 0.0] \t\t Generator Loss: 0.016178488731384277\n",
      "Epoch: 419 \t Discriminator Loss: [2.2310073, 0.0] \t\t Generator Loss: 0.016146350651979446\n",
      "Epoch: 420 \t Discriminator Loss: [2.231814, 0.0] \t\t Generator Loss: 0.016120674088597298\n",
      "Epoch: 421 \t Discriminator Loss: [2.232947, 0.0] \t\t Generator Loss: 0.01608235388994217\n",
      "Epoch: 422 \t Discriminator Loss: [2.2338576, 0.0] \t\t Generator Loss: 0.01605277881026268\n",
      "Epoch: 423 \t Discriminator Loss: [2.2348897, 0.0] \t\t Generator Loss: 0.016019336879253387\n",
      "Epoch: 424 \t Discriminator Loss: [2.2358608, 0.0] \t\t Generator Loss: 0.015988662838935852\n",
      "Epoch: 425 \t Discriminator Loss: [2.2367148, 0.0] \t\t Generator Loss: 0.015961088240146637\n",
      "Epoch: 426 \t Discriminator Loss: [2.2377715, 0.0] \t\t Generator Loss: 0.01592632755637169\n",
      "Epoch: 427 \t Discriminator Loss: [2.2387822, 0.0] \t\t Generator Loss: 0.015894869342446327\n",
      "Epoch: 428 \t Discriminator Loss: [2.2396731, 0.0] \t\t Generator Loss: 0.01586524024605751\n",
      "Epoch: 429 \t Discriminator Loss: [2.2406993, 0.0] \t\t Generator Loss: 0.015832390636205673\n",
      "Epoch: 430 \t Discriminator Loss: [2.2416234, 0.0] \t\t Generator Loss: 0.015802841633558273\n",
      "Epoch: 431 \t Discriminator Loss: [2.2425356, 0.0] \t\t Generator Loss: 0.015774454921483994\n",
      "Epoch: 432 \t Discriminator Loss: [2.2435374, 0.0] \t\t Generator Loss: 0.015741851180791855\n",
      "Epoch: 433 \t Discriminator Loss: [2.2444255, 0.0] \t\t Generator Loss: 0.015713617205619812\n",
      "Epoch: 434 \t Discriminator Loss: [2.2454667, 0.0] \t\t Generator Loss: 0.015680603682994843\n",
      "Epoch: 435 \t Discriminator Loss: [2.2463934, 0.0] \t\t Generator Loss: 0.015651270747184753\n",
      "Epoch: 436 \t Discriminator Loss: [2.2473724, 0.0] \t\t Generator Loss: 0.01562037318944931\n",
      "Epoch: 437 \t Discriminator Loss: [2.2483, 0.0] \t\t Generator Loss: 0.015591097995638847\n",
      "Epoch: 438 \t Discriminator Loss: [2.2492027, 0.0] \t\t Generator Loss: 0.015562694519758224\n",
      "Epoch: 439 \t Discriminator Loss: [2.250219, 0.0] \t\t Generator Loss: 0.015531782060861588\n",
      "Epoch: 440 \t Discriminator Loss: [2.2510552, 0.0] \t\t Generator Loss: 0.015504549257457256\n",
      "Epoch: 441 \t Discriminator Loss: [2.2520902, 0.0] \t\t Generator Loss: 0.015473190695047379\n",
      "Epoch: 442 \t Discriminator Loss: [2.252933, 0.0] \t\t Generator Loss: 0.015446560457348824\n",
      "Epoch: 443 \t Discriminator Loss: [2.2540574, 0.0] \t\t Generator Loss: 0.015413833782076836\n",
      "Epoch: 444 \t Discriminator Loss: [2.2548256, 0.0] \t\t Generator Loss: 0.015386899933218956\n",
      "Epoch: 445 \t Discriminator Loss: [2.255764, 0.0] \t\t Generator Loss: 0.015357759781181812\n",
      "Epoch: 446 \t Discriminator Loss: [2.2567396, 0.0] \t\t Generator Loss: 0.015327530913054943\n",
      "Epoch: 447 \t Discriminator Loss: [2.2576525, 0.0] \t\t Generator Loss: 0.015299294143915176\n",
      "Epoch: 448 \t Discriminator Loss: [2.2586298, 0.0] \t\t Generator Loss: 0.01526913233101368\n",
      "Epoch: 449 \t Discriminator Loss: [2.2595005, 0.0] \t\t Generator Loss: 0.015243694186210632\n",
      "Epoch: 450 \t Discriminator Loss: [2.2604752, 0.0] \t\t Generator Loss: 0.015212208032608032\n",
      "Epoch: 451 \t Discriminator Loss: [2.2613044, 0.0] \t\t Generator Loss: 0.0151868537068367\n",
      "Epoch: 452 \t Discriminator Loss: [2.2622309, 0.0] \t\t Generator Loss: 0.015159161761403084\n",
      "Epoch: 453 \t Discriminator Loss: [2.2632117, 0.0] \t\t Generator Loss: 0.015129035338759422\n",
      "Epoch: 454 \t Discriminator Loss: [2.264121, 0.0] \t\t Generator Loss: 0.015100784599781036\n",
      "Epoch: 455 \t Discriminator Loss: [2.2650707, 0.0] \t\t Generator Loss: 0.015071775764226913\n",
      "Epoch: 456 \t Discriminator Loss: [2.265977, 0.0] \t\t Generator Loss: 0.015045207925140858\n",
      "Epoch: 457 \t Discriminator Loss: [2.2669823, 0.0] \t\t Generator Loss: 0.015015671029686928\n",
      "Epoch: 458 \t Discriminator Loss: [2.2677302, 0.0] \t\t Generator Loss: 0.014991031028330326\n",
      "Epoch: 459 \t Discriminator Loss: [2.2686276, 0.0] \t\t Generator Loss: 0.014963904395699501\n",
      "Epoch: 460 \t Discriminator Loss: [2.2695475, 0.0] \t\t Generator Loss: 0.014936119318008423\n",
      "Epoch: 461 \t Discriminator Loss: [2.2704668, 0.0] \t\t Generator Loss: 0.014908412471413612\n",
      "Epoch: 462 \t Discriminator Loss: [2.2713692, 0.0] \t\t Generator Loss: 0.014881400391459465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 463 \t Discriminator Loss: [2.2722557, 0.0] \t\t Generator Loss: 0.014854665845632553\n",
      "Epoch: 464 \t Discriminator Loss: [2.2731805, 0.0] \t\t Generator Loss: 0.01482790894806385\n",
      "Epoch: 465 \t Discriminator Loss: [2.2741032, 0.0] \t\t Generator Loss: 0.01479934249073267\n",
      "Epoch: 466 \t Discriminator Loss: [2.2750268, 0.0] \t\t Generator Loss: 0.014771774411201477\n",
      "Epoch: 467 \t Discriminator Loss: [2.2758677, 0.0] \t\t Generator Loss: 0.014746706932783127\n",
      "Epoch: 468 \t Discriminator Loss: [2.2767286, 0.0] \t\t Generator Loss: 0.014721091836690903\n",
      "Epoch: 469 \t Discriminator Loss: [2.2776608, 0.0] \t\t Generator Loss: 0.014693409204483032\n",
      "Epoch: 470 \t Discriminator Loss: [2.2785192, 0.0] \t\t Generator Loss: 0.014667950570583344\n",
      "Epoch: 471 \t Discriminator Loss: [2.2794423, 0.0] \t\t Generator Loss: 0.01464064884930849\n",
      "Epoch: 472 \t Discriminator Loss: [2.280418, 0.0] \t\t Generator Loss: 0.01461281068623066\n",
      "Epoch: 473 \t Discriminator Loss: [2.2811997, 0.0] \t\t Generator Loss: 0.014590159058570862\n",
      "Epoch: 474 \t Discriminator Loss: [2.282086, 0.0] \t\t Generator Loss: 0.014562711119651794\n",
      "Epoch: 475 \t Discriminator Loss: [2.283046, 0.0] \t\t Generator Loss: 0.014534525573253632\n",
      "Epoch: 476 \t Discriminator Loss: [2.2838664, 0.0] \t\t Generator Loss: 0.01451046671718359\n",
      "Epoch: 477 \t Discriminator Loss: [2.2847142, 0.0] \t\t Generator Loss: 0.014485647901892662\n",
      "Epoch: 478 \t Discriminator Loss: [2.2856197, 0.0] \t\t Generator Loss: 0.014459196478128433\n",
      "Epoch: 479 \t Discriminator Loss: [2.2865112, 0.0] \t\t Generator Loss: 0.014433203265070915\n",
      "Epoch: 480 \t Discriminator Loss: [2.2874537, 0.0] \t\t Generator Loss: 0.01440767478197813\n",
      "Epoch: 481 \t Discriminator Loss: [2.288231, 0.0] \t\t Generator Loss: 0.014383175410330296\n",
      "Epoch: 482 \t Discriminator Loss: [2.28913, 0.0] \t\t Generator Loss: 0.014357096515595913\n",
      "Epoch: 483 \t Discriminator Loss: [2.2899523, 0.0] \t\t Generator Loss: 0.014333274215459824\n",
      "Epoch: 484 \t Discriminator Loss: [2.2908716, 0.0] \t\t Generator Loss: 0.014306703582406044\n",
      "Epoch: 485 \t Discriminator Loss: [2.291746, 0.0] \t\t Generator Loss: 0.014281485229730606\n",
      "Epoch: 486 \t Discriminator Loss: [2.2925773, 0.0] \t\t Generator Loss: 0.014257530681788921\n",
      "Epoch: 487 \t Discriminator Loss: [2.293444, 0.0] \t\t Generator Loss: 0.014232607558369637\n",
      "Epoch: 488 \t Discriminator Loss: [2.294279, 0.0] \t\t Generator Loss: 0.014208635315299034\n",
      "Epoch: 489 \t Discriminator Loss: [2.2951608, 0.0] \t\t Generator Loss: 0.014184027910232544\n",
      "Epoch: 490 \t Discriminator Loss: [2.2960181, 0.0] \t\t Generator Loss: 0.014158839359879494\n",
      "Epoch: 491 \t Discriminator Loss: [2.2968779, 0.0] \t\t Generator Loss: 0.014134285971522331\n",
      "Epoch: 492 \t Discriminator Loss: [2.2977753, 0.0] \t\t Generator Loss: 0.014108707197010517\n",
      "Epoch: 493 \t Discriminator Loss: [2.298596, 0.0] \t\t Generator Loss: 0.01408538967370987\n",
      "Epoch: 494 \t Discriminator Loss: [2.29951, 0.0] \t\t Generator Loss: 0.014059406705200672\n",
      "Epoch: 495 \t Discriminator Loss: [2.3003416, 0.0] \t\t Generator Loss: 0.014037135057151318\n",
      "Epoch: 496 \t Discriminator Loss: [2.301126, 0.0] \t\t Generator Loss: 0.014013612642884254\n",
      "Epoch: 497 \t Discriminator Loss: [2.302017, 0.0] \t\t Generator Loss: 0.013988430611789227\n",
      "Epoch: 498 \t Discriminator Loss: [2.3028843, 0.0] \t\t Generator Loss: 0.013963967561721802\n",
      "Epoch: 499 \t Discriminator Loss: [2.3037474, 0.0] \t\t Generator Loss: 0.013940323144197464\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar  4 12:10:56 2020\n",
    "\n",
    "@author: gracehymas\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.layers import (BatchNormalization, Conv2D, Conv2DTranspose, Dense,\n",
    "                          Dropout, Flatten, Input, Reshape, UpSampling2D,\n",
    "                          ZeroPadding2D)\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import *\n",
    "from keras.optimizers import SGD, Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import History\n",
    "from keras import metrics\n",
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../dataset-and-plotting\")\n",
    "from nnPlotting import *\n",
    "from sensitivity import *\n",
    "\n",
    "def totalSensitivity(A,B,errorA,errorB):\n",
    "    totalSensitivity = np.sqrt(A**2 + B**2)\n",
    "    totalError = np.sqrt(((A*errorA)/np.sqrt(A**2 + B**2))**2 + ((B*errorB)/np.sqrt(A**2 + B**2))**2)\n",
    "\n",
    "    return (totalSensitivity,totalError)\n",
    "\n",
    "dataStorage = np.zeros((500, 7))\n",
    "\n",
    "\n",
    "def create_generator():\n",
    "    \n",
    "    generator = Sequential()\n",
    "    \n",
    "    generator.add(Dense(30, activation=\"tanh\",input_dim=noise_dim))\n",
    "    generator.add(Dense(2, activation=\"softmax\"))\n",
    "    \n",
    "    generator.add(Dense(len(variables), activation='tanh'))\n",
    "                  \n",
    "    generator.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    \n",
    "    return generator\n",
    "\n",
    "\n",
    "def create_descriminator():\n",
    "    \n",
    "    discriminator = Sequential()\n",
    "    \n",
    "    discriminator.add(Dense(40, activation=\"linear\", input_dim=len(variables)))\n",
    "    discriminator.add(Dense(40, activation=\"tanh\"))\n",
    "    discriminator.add(Dense(40, activation=\"tanh\"))\n",
    "    \n",
    "    discriminator.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimiserClassifier, metrics=['binary_accuracy'])\n",
    "    \n",
    "    return discriminator\n",
    "\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "variables = ['pTB1','pTB2']\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "dfEven = pd.read_csv('../dataset-and-plotting/CSV/VHbb_data_2jet_even.csv')\n",
    "dfOdd = pd.read_csv('../dataset-and-plotting/CSV/VHbb_data_2jet_odd.csv')\n",
    "\n",
    "df = pd.concat([dfEven,dfOdd])\n",
    "\n",
    "dfTrain = df.loc[df['category'] == 'VH']\n",
    "x_train_array = dfTrain[variables].to_numpy()\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#x_train = scaler.fit_transform(x_train_array)\n",
    "x_train = x_train_array\n",
    "\n",
    "batch_size = 64\n",
    "steps_per_epoch = int(len(dfTrain) / batch_size)\n",
    "\n",
    "noise_dim = 100\n",
    "epochs = 500\n",
    "\n",
    "optimizer = SGD(0.01)\n",
    "\n",
    "optimiserClassifier = SGD(lr=0.001, momentum=0.5, decay=0.00001)\n",
    "\n",
    "discriminator = create_descriminator()\n",
    "generator = create_generator()\n",
    "\n",
    "# Link the two models to create the GAN\n",
    "gan_input = Input(shape=(noise_dim,))\n",
    "\n",
    "fake_event = generator(gan_input)\n",
    "\n",
    "gan_output = discriminator(fake_event)\n",
    "\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "dloss = []\n",
    "gloss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(steps_per_epoch):\n",
    "        \n",
    "        noise = np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
    "        fake_x = generator.predict(noise)\n",
    "\n",
    "        real_x = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
    "        \n",
    "        x = np.concatenate((real_x, fake_x))\n",
    "\n",
    "        # First half of y-values are 0.9 corresponding to real data, not using 1 for label smoothing\n",
    "        # Second half are 0 corresponding to the fake data we made\n",
    "        disc_y = np.zeros(2*batch_size)\n",
    "        disc_y[:batch_size] = 0.9\n",
    "        \n",
    "        discriminator.trainable = True\n",
    "        d_loss = discriminator.train_on_batch(x, disc_y)\n",
    "\n",
    "        discriminator.trainable = False\n",
    "        y_gen = np.ones(batch_size)\n",
    "        g_loss = gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "    dloss.append(d_loss[0])\n",
    "    gloss.append(g_loss)   \n",
    "    print(f'Epoch: {epoch} \\t Discriminator Loss: {d_loss} \\t\\t Generator Loss: {g_loss}')\n",
    "\n",
    "df = pd.DataFrame(np.array([dloss,gloss]).T, columns = ['discriminator', 'generator'])\n",
    "df.to_csv(\"ThisIsLossSimpleLong.csv\")\n",
    "\n",
    "discriminator.save('Discriminator.hdf5')\n",
    "generator.save('Generator.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:384: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 62        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 3,098\n",
      "Trainable params: 3,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.layers import (BatchNormalization, Conv2D, Conv2DTranspose, Dense,\n",
    "                          Dropout, Flatten, Input, Reshape, UpSampling2D,\n",
    "                          ZeroPadding2D)\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import *\n",
    "from keras.optimizers import SGD, Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import History\n",
    "from keras import metrics\n",
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../dataset-and-plotting\")\n",
    "from nnPlotting import *\n",
    "from sensitivity import *\n",
    "\n",
    "from keras.models import load_model\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import os\n",
    "\n",
    "variables = ['pTB1','pTB2']\n",
    "\n",
    "dfEven = pd.read_csv('../dataset-and-plotting/CSV/VHbb_data_2jet_even.csv')\n",
    "dfOdd = pd.read_csv('../dataset-and-plotting/CSV/VHbb_data_2jet_odd.csv')\n",
    "\n",
    "df = pd.concat([dfEven,dfOdd])\n",
    "\n",
    "dfTrain = df.loc[df['category'] == 'VH']\n",
    "x_train_array = dfTrain[variables].to_numpy()\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#x_train = scaler.fit_transform(x_train_array)\n",
    "x_train = x_train_array\n",
    "\n",
    "generator = load_model('Generator.hdf5')\n",
    "print(generator.summary())\n",
    "\n",
    "GAN_noise_size = 100\n",
    "n_events = 10000\n",
    "\n",
    "X_noise = np.random.normal(0, 1, size=(n_events, GAN_noise_size))\n",
    "\n",
    "X_generated = generator.predict(X_noise)\n",
    "\n",
    "#X_unscaled = scaler.inverse_transform(X_generated)\n",
    "\n",
    "Events = pd.DataFrame(X_generated)\n",
    "Events.to_csv('GeneratedEvents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pTB1 = Events.loc[df['category'] == 'pTB1']\n",
    "#pTB2 = df.loc[df['category'] == 'pTB2']\n",
    "\n",
    "plt.hist(Events[0], stacked=True, alpha=0.75, bins = 200)\n",
    "plt.show()\n",
    "plt.hist(Events[1], density=True, alpha=0.75, bins = 200)\n",
    "plt.show()\n",
    "\n",
    "dfEven = pd.read_csv('../dataset-and-plotting/CSV/VHbb_data_2jet_even.csv')\n",
    "dfOdd = pd.read_csv('../dataset-and-plotting/CSV/VHbb_data_2jet_odd.csv')\n",
    "\n",
    "df = pd.concat([dfEven,dfOdd])\n",
    "\n",
    "real_events = df.loc[df['category'] == 'VH']['pTB2']\n",
    "\n",
    "plt.hist(real_events, density=True, alpha=0.75, bins = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
